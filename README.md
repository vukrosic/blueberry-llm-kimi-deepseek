# Zero to AI Researcher: A Complete Learning Journey

Welcome to **Zero to AI Researcher** - a comprehensive, hands-on course that takes you from absolute beginner to conducting cutting-edge AI research. This course combines theoretical foundations with practical implementation, culminating in real research experiments with state-of-the-art transformer architectures.

## ğŸ¯ Course Overview

This course is designed for anyone who wants to understand AI from the ground up and eventually conduct their own research. Whether you're a complete beginner or have some programming experience, this structured learning path will guide you through:

- **Programming Fundamentals**: Python from scratch
- **Mathematical Foundations**: The math behind AI (made approachable!)
- **Deep Learning Basics**: Neural networks from first principles
- **Modern AI Architectures**: Transformers, attention mechanisms, and MoE
- **Research Methodology**: How to design and conduct AI experiments
- **Real Research**: Hands-on experiments with advanced models

## ğŸ“š Complete Curriculum

### ğŸš€ Getting Started
- **[Start Here](_course/00_start_here/00_start_here.md)** - Course introduction and learning philosophy

### ğŸ“– Module 1: Python Fundamentals
*Master the programming language that powers AI*

- **[Python Basics](_course/01_python_beginner_lessons/01_python_basics.md)** - Variables, data types, and functions
- **[Control Flow and Loops](_course/01_python_beginner_lessons/02_control_flow_and_loops.md)** - If statements, loops, and program flow
- **[Lists and Data Structures](_course/01_python_beginner_lessons/03_lists_and_data_structures.md)** - Lists, dictionaries, tuples, and sets
- **[File Handling and Modules](_course/01_python_beginner_lessons/04_file_handling_and_modules.md)** - Working with files and Python modules
- **[Error Handling and Debugging](_course/01_python_beginner_lessons/05_error_handling_and_debugging.md)** - Exception handling and debugging techniques
- **[Object-Oriented Programming](_course/01_python_beginner_lessons/06_object_oriented_programming.md)** - Classes, inheritance, and OOP concepts
- **[Advanced Python Features](_course/01_python_beginner_lessons/07_advanced_python_features.md)** - Decorators, generators, and context managers
- **[Preparing for AI/ML](_course/01_python_beginner_lessons/08_preparing_for_ai_ml.md)** - NumPy, Pandas, Matplotlib, and Scikit-learn
- **[Python Best Practices](_course/01_python_beginner_lessons/09_python_best_practices.md)** - Code quality, testing, and performance

### ğŸ§® Module 2: Math Not Scary
*The mathematical foundations of AI, explained simply*

- **[Functions](_course/02_math_not_scary/01_functions.md)** - Understanding mathematical functions
- **[Derivatives](_course/02_math_not_scary/02_derivatives.md)** - The foundation of optimization
- **[Gradients](_course/02_math_not_scary/03_gradients.md)** - Multi-dimensional derivatives
- **[Vectors](_course/02_math_not_scary/04_vectors.md)** - Vector operations and properties
- **[Matrices](_course/02_math_not_scary/05_matrices.md)** - Matrix operations and linear algebra

### ğŸ”¥ Module 3: PyTorch Fundamentals
*Master the deep learning framework*

- **[Creating Tensors](_course/03_pytorch_fundamentals/01_creating_tensors.md)** - The building blocks of deep learning
- **[Tensor Addition](_course/03_pytorch_fundamentals/02_tensor_addition.md)** - Basic tensor operations
- **[Matrix Multiplication](_course/03_pytorch_fundamentals/03_matrix_multiplication.md)** - The core operation of neural networks
- **[Transposing](_course/03_pytorch_fundamentals/04_transposing.md)** - Reshaping tensors for operations
- **[Reshaping Tensors](_course/03_pytorch_fundamentals/05_reshaping_tensors.md)** - Changing tensor dimensions
- **[Indexing and Slicing](_course/03_pytorch_fundamentals/06_indexing_and_slicing.md)** - Accessing tensor elements
- **[Concatenation](_course/03_pytorch_fundamentals/07_concatenation.md)** - Combining tensors
- **[Creating Special Tensors](_course/03_pytorch_fundamentals/08_creating_special_tensors.md)** - Ones, zeros, and random tensors
- **[Tokenization and Embeddings](_course/03_pytorch_fundamentals/09_tokenization_and_embeddings.md)** - Converting text to numbers

### ğŸ§  Module 4: Neuron from Scratch
*Understanding the basic building block of AI*

- **[What is a Neuron?](_course/04_neuron_from_scratch/01_what_is_a_neuron.md)** - The fundamental unit of neural networks
- **[The Linear Step](_course/04_neuron_from_scratch/02_the_linear_step.md)** - Weighted sum computation
- **[The Activation Function](_course/04_neuron_from_scratch/03_the_activation_function.md)** - Adding non-linearity
- **[Building a Neuron in Python](_course/04_neuron_from_scratch/04_building_a_neuron_in_python.md)** - Implementation from scratch
- **[Making a Prediction](_course/04_neuron_from_scratch/05_making_a_prediction.md)** - Using neurons for inference
- **[The Concept of Loss](_course/04_neuron_from_scratch/06_the_concept_of_loss.md)** - Measuring prediction errors
- **[The Concept of Learning](_course/04_neuron_from_scratch/07_the_concept_of_learning.md)** - How neurons improve over time

### âš¡ Module 5: Activation Functions
*The non-linear functions that make neural networks powerful*

- **[ReLU](_course/05_activation_functions/01_relu.md)** - Rectified Linear Unit
- **[Sigmoid](_course/05_activation_functions/02_sigmoid.md)** - The S-shaped function
- **[Tanh](_course/05_activation_functions/03_tanh.md)** - Hyperbolic tangent
- **[SiLU](_course/05_activation_functions/04_silu.md)** - Sigmoid Linear Unit
- **[SwiGLU](_course/05_activation_functions/05_swiglu.md)** - Swish-Gated Linear Unit
- **[Softmax](_course/05_activation_functions/06_softmax.md)** - Probability distributions

### ğŸ•¸ï¸ Module 6: Neural Network from Scratch
*Building complete networks and understanding backpropagation*

- **[Architecture of a Network](_course/06_neural_network_from_scratch/01_architecture_of_a_network.md)** - How layers connect
- **[Building a Layer](_course/06_neural_network_from_scratch/02_building_a_layer.md)** - Implementing network layers
- **[Implementing a Network](_course/06_neural_network_from_scratch/03_implementing_a_network.md)** - Complete network implementation
- **[The Chain Rule](_course/06_neural_network_from_scratch/04_the_chain_rule.md)** - Mathematical foundation of backpropagation
- **[Calculating Gradients](_course/06_neural_network_from_scratch/05_calculating_gradients.md)** - Computing derivatives
- **[Backpropagation in Action](_course/06_neural_network_from_scratch/06_backpropagation_in_action.md)** - How networks learn
- **[Implementing Backpropagation](_course/06_neural_network_from_scratch/07_implementing_backpropagation.md)** - Code implementation

### ğŸ¯ Module 7: Attention Mechanism
*The breakthrough that revolutionized AI*

- **[What is Attention?](_course/07_attention_mechanism/01_what_is_attention.md)** - Understanding the attention concept
- **[Self-Attention from Scratch](_course/07_attention_mechanism/02_self_attention_from_scratch.md)** - Building attention step by step
- **[Calculating Attention Scores](_course/07_attention_mechanism/03_calculating_attention_scores.md)** - Query, key, and value operations
- **[Applying Attention Weights](_course/07_attention_mechanism/04_applying_attention_weights.md)** - Weighted combinations
- **[Multi-Head Attention](_course/07_attention_mechanism/05_multi_head_attention.md)** - Multiple attention mechanisms
- **[Attention in Code](_course/07_attention_mechanism/06_attention_in_code.md)** - Complete implementation

### ğŸ”„ Module 8: Transformer Feedforward
*The feedforward layers and Mixture of Experts*

- **[The Feedforward Layer](_course/08_transformer_feedforward/01_the_feedforward_layer.md)** - Standard MLP layers
- **[What is Mixture of Experts?](_course/08_transformer_feedforward/02_what_is_mixture_of_experts.md)** - Introduction to MoE
- **[The Expert](_course/08_transformer_feedforward/03_the_expert.md)** - Individual expert networks
- **[The Gate](_course/08_transformer_feedforward/04_the_gate.md)** - Expert selection mechanism
- **[Combining Experts](_course/08_transformer_feedforward/05_combining_experts.md)** - Weighted expert outputs
- **[MoE in a Transformer](_course/08_transformer_feedforward/06_moe_in_a_transformer.md)** - Integration with attention
- **[MoE in Code](_course/08_transformer_feedforward/07_moe_in_code.md)** - Implementation
- **[The DeepSeek MLP](_course/08_transformer_feedforward/08_the_deepseek_mlp.md)** - Advanced MLP design

### ğŸ—ï¸ Module 9: Building a Transformer
*Assembling the complete architecture*

- **[Transformer Architecture](_course/09_building_a_transformer/01_transformer_architecture.md)** - High-level overview
- **[RoPE Positional Encoding](_course/09_building_a_transformer/02_rope_positional_encoding.md)** - Rotary positional embeddings
- **[Building a Transformer Block](_course/09_building_a_transformer/03_building_a_transformer_block.md)** - Attention + feedforward
- **[The Final Linear Layer](_course/09_building_a_transformer/05_the_final_linear_layer.md)** - Output projection
- **[Full Transformer in Code](_course/09_building_a_transformer/06_full_transformer_in_code.md)** - Complete implementation
- **[Training a Transformer](_course/09_building_a_transformer/07_training_a_transformer.md)** - Training process overview

### ğŸš€ Module 10: DeepSeek Latent Attention
*Advanced attention mechanisms from DeepSeek models*

- **[What is Latent Attention?](_course/10_deepseek_latent_attention/01_what_is_latent_attention.md)** - Understanding latent attention
- **[DeepSeek Attention Architecture](_course/10_deepseek_latent_attention/02_deepseek_attention_architecture.md)** - DeepSeek's specific design
- **[Implementation in Code](_course/10_deepseek_latent_attention/03_implementation_in_code.md)** - Building DeepSeek attention

### ğŸ­ Module 11: GLM-4 Mixture of Experts
*State-of-the-art MoE implementation*

- **[Revisiting Mixture of Experts](_course/11_glm4_moe/01_revisiting_mixture_of_experts.md)** - MoE fundamentals recap
- **[The GLM-4 MoE Architecture](_course/11_glm4_moe/02_the_glm4_moe_architecture.md)** - GLM-4's MoE design
- **[Implementation in Code](_course/11_glm4_moe/03_implementation_in_code.md)** - Building GLM-4 MoE

## ğŸ”¬ Research Methodology: How to Conduct AI Research

After mastering the fundamentals, this course teaches you how to conduct real AI research through hands-on experiments. Here's how we approach research:

### 1. **Research Design Principles**
- **Hypothesis Formation**: Start with clear, testable hypotheses
- **Controlled Experiments**: Isolate variables to understand their effects
- **Ablation Studies**: Systematically remove components to understand contributions
- **Baseline Comparisons**: Always compare against established baselines

### 2. **Experimental Framework**
Our research experiments follow a structured approach:

#### **Experiment 1: Simplified Ablation Study**
- **Purpose**: Compare different architectural components at a manageable scale
- **Models**: 5 variants (baseline, MLP, attention+MLP, MoE, attention+MoE)
- **Scale**: 512 hidden dimensions for efficient experimentation
- **Evaluation**: HellaSwag benchmark integration
- **Key Learning**: Understanding how different components contribute to performance

#### **Experiment 2: Learning Rate Search**
- **Purpose**: Find optimal learning rates for different architectures
- **Focus**: DeepSeek attention + MLP combinations
- **Method**: Systematic learning rate exploration
- **Metrics**: Validation loss, accuracy, perplexity
- **Key Learning**: How hyperparameters affect different architectures

#### **Experiment 3: Expert Configuration Search**
- **Purpose**: Optimize MoE configurations
- **Focus**: DeepSeek attention + GLM4 MoE
- **Variables**: Expert count, learning rates, top-k values
- **Method**: Grid search with validation
- **Key Learning**: How to scale MoE models effectively

### 3. **Research Skills You'll Develop**
- **Experimental Design**: Creating meaningful, controlled experiments
- **Data Analysis**: Interpreting results and drawing conclusions
- **Benchmarking**: Using standard evaluation metrics
- **Reproducibility**: Writing code that others can replicate
- **Documentation**: Communicating research findings clearly

### 4. **How to Run the Research Experiments**

```bash
# Experiment 1: Simplified Ablation Study
cd experiments/exp1_simplified_ablation_study
python exp1_trainer.py

# Experiment 2: Learning Rate Search
cd experiments/exp2_deepseek_attn_mlp_lr_search
python lr_search.py

# Experiment 3: Expert Configuration Search
cd experiments/exp3_deepseek_attn_glm4_moe_lr_expert_search
python expert_search.py
```

## ğŸ—ï¸ Repository Structure

This repository serves as both a learning resource and a research framework:

```
zero-to-ai-researcher/
â”œâ”€â”€ _course/                          # Complete course materials
â”‚   â”œâ”€â”€ 00_start_here/               # Course introduction
â”‚   â”œâ”€â”€ 01_python_beginner_lessons/  # Python fundamentals
â”‚   â”œâ”€â”€ 02_math_not_scary/           # Mathematical foundations
â”‚   â”œâ”€â”€ 03_pytorch_fundamentals/     # PyTorch basics
â”‚   â”œâ”€â”€ 04_neuron_from_scratch/      # Neural network basics
â”‚   â”œâ”€â”€ 05_activation_functions/     # Activation functions
â”‚   â”œâ”€â”€ 06_neural_network_from_scratch/ # Neural networks
â”‚   â”œâ”€â”€ 07_attention_mechanism/      # Attention mechanisms
â”‚   â”œâ”€â”€ 08_transformer_feedforward/  # Transformer FF layers
â”‚   â”œâ”€â”€ 09_building_a_transformer/   # Full transformer
â”‚   â”œâ”€â”€ 10_deepseek_latent_attention/ # DeepSeek attention
â”‚   â””â”€â”€ 11_glm4_moe/                 # GLM4 MoE architecture
â”œâ”€â”€ experiments/                      # Research experiments
â”‚   â”œâ”€â”€ exp1_simplified_ablation_study/ # Architecture comparison
â”‚   â”œâ”€â”€ exp2_deepseek_attn_mlp_lr_search/ # Learning rate optimization
â”‚   â””â”€â”€ exp3_deepseek_attn_glm4_moe_lr_expert_search/ # MoE optimization
â”œâ”€â”€ models/                          # Model implementations
â”‚   â”œâ”€â”€ components.py                # Model components
â”‚   â”œâ”€â”€ layers.py                    # Custom layers
â”‚   â””â”€â”€ moe_llm.py                   # MoE LLM implementation
â”œâ”€â”€ training/                        # Training utilities
â”‚   â”œâ”€â”€ trainer.py                   # Training loop
â”‚   â””â”€â”€ evaluation.py                # Evaluation metrics
â”œâ”€â”€ data/                            # Data handling
â”‚   â”œâ”€â”€ dataset.py                   # Dataset classes
â”‚   â””â”€â”€ loader.py                    # Data loading utilities
â”œâ”€â”€ optimizers/                      # Custom optimizers
â”‚   â””â”€â”€ muon.py                      # Muon optimizer
â”œâ”€â”€ configs/                         # Configuration files
â”‚   â””â”€â”€ moe_config.py                # MoE model configuration
â””â”€â”€ utils/                           # Utility functions
    â””â”€â”€ helpers.py                   # Helper functions
```

## ğŸš€ Getting Started

### Prerequisites
- No programming experience required
- Basic computer skills
- Access to a computer with internet

### Installation

1. **Clone the repository**:
```bash
git clone <repository-url>
cd zero-to-ai-researcher
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Verify installation**:
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

### Learning Path

1. **Start with the Course**: Begin with [Start Here](_course/00_start_here/00_start_here.md)
2. **Follow the Modules**: Complete modules 1-11 in order
3. **Practice with Code**: Implement each concept as you learn
4. **Run Experiments**: Conduct the research experiments
5. **Build Your Own**: Create your own research projects

## ğŸ“ Learning Outcomes

By completing this course, you will:

- **Master Python Programming**: From basics to advanced features
- **Understand AI Mathematics**: Functions, derivatives, gradients, linear algebra
- **Build Neural Networks**: From single neurons to complex architectures
- **Implement Transformers**: Complete transformer models from scratch
- **Conduct Research**: Design and run meaningful AI experiments
- **Work with State-of-the-Art**: DeepSeek attention and GLM-4 MoE
- **Evaluate Models**: Use standard benchmarks and metrics
- **Document Research**: Communicate findings effectively

## ğŸ”¬ Research Applications

This course prepares you for:

- **Academic Research**: Conducting publishable AI research
- **Industry Applications**: Building production AI systems
- **Startup Development**: Creating AI-powered products
- **Open Source Contributions**: Contributing to AI projects
- **Teaching**: Sharing knowledge with others

## ğŸ¤ Contributing

We welcome contributions to improve this course:

- **Content Improvements**: Better explanations, examples, or exercises
- **New Modules**: Additional topics or advanced concepts
- **Research Experiments**: New experimental designs
- **Documentation**: Clearer instructions or additional resources
- **Bug Fixes**: Code corrections or improvements

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **DeepSeek**: For the advanced attention architecture
- **GLM-4**: For the MoE implementation inspiration
- **HuggingFace**: For the transformer library foundation
- **PyTorch**: For the deep learning framework
- **OpenAI**: For the transformer architecture
- **Google**: For the attention mechanism

## ğŸ“ Support and Community

- **GitHub Issues**: Report problems or suggest improvements
- **Discussions**: Connect with other learners
- **Code Reviews**: Get feedback on your implementations
- **Research Collaboration**: Work together on experiments

---

**Ready to start your journey from zero to AI researcher?** Begin with [Start Here](_course/00_start_here/00_start_here.md) and remember: every expert was once a beginner. Take your time, practice regularly, and don't hesitate to experiment!

**Happy Learning and Researching! ğŸš€ğŸ§ **
